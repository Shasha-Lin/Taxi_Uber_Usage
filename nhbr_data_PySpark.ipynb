{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "from pyspark import SparkContext\n",
    "import sys\n",
    "import re\n",
    "from itertools import islice\n",
    "from calendar import weekday\n",
    "\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "lines = sc.textFile(sys.argv[1],1)\n",
    "\n",
    "header = lines.first()\n",
    "lines = lines.filter(lambda line: line != header)\n",
    "\n",
    "weekend = [5,6]\n",
    "\n",
    "tip = 13\n",
    "fare = 10\n",
    "total_amount = 16\n",
    "pu = 7\n",
    "do = 8\n",
    "passengers = 3\n",
    "\n",
    "#filter entries with no  loc_id\n",
    "#first rdd format: ( pick up loc_id ' ' DO loc_id, (total amount paid, # passengers, 1  )   ) \n",
    "# reduce by PU, DO key\n",
    "#output rdd format:\n",
    "# [ PU loc_id, DO loc_id, total amount paid, # passengers,  # entries in this PU-DO combo     ]\n",
    "# call flatten on this and put into pandas\n",
    "\n",
    "lines = lines.mapPartitions(lambda l: reader(l)) \\\n",
    "        .map(lambda l: ( l[pu]  +' '+ l[do]  , (l[total_amount], l[passengers], 1 ) )                    )\\\n",
    "        .reduceByKey(lambda v1, v2: (float(v1[0])+float(v2[0]), float(v1[1])+float(v2[1]), float(v1[2])+float(v2[2]) ) ) \\\n",
    "        .map(lambda l: [int(l[0].split()[0]), int(l[0].split()[1]), float(l[1][0]), int(l[1][1]), int(l[1][2])  ] )\\\n",
    "\n",
    "lines.saveAsTextFile(\"yellow_2016\")\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
